---
title: "users con API académica"
output:
  html_document:
    df_print: paged
params:
    dataset_name : "xxxxxxxxxxx"         # Nombre del dataset (el directorio debe estar creado)
    file_users   : "xxxxxxxxxxx"         # fichero con la lista de usuarios 
    search       : "profile | favorites | followers | following "  # datos que se quieren bajar, elegir sólo una opción
---

```{r setup, 	echo = TRUE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}

require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")
```

## Entorno de trabajo

Estos notebooks trabajan con esta estructura de directorios

    dir_raiz ----+-----data      # Se guardan los datos, cada dataset en un directorio independiente
                 |
                 +-----keys      # se guardan los ficheros con las claves de acceso. 
                 |
                 +-----notebooks # Se guardan los notebook en R
                 

## Requisitos

-   Disponer de una app Académica
-   Configurar la clave de acceso por defecto. Solo es necesario hacerlo una vez con el cuaderno cfg_API_academic.Rmd. Esto permite comprobar que la calve fuiona y evita tener que introducirla como parámetro en los otros cuadernos.

## Importamos librerías

```{r libraries}
if (!"academictwitteR" %in% installed.packages()) {install.packages("academictwitteR")}
if (!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if (!"lubridate" %in% installed.packages()) {install.packages("lubridate")}
if (!"base" %in% installed.packages()) {install.packages("base")}
if (!"svDialogs" %in% installed.packages()) {install.packages("svDialogs")}
library(base)             # Librerías base de R
library(academictwitteR)  # Interfaz con API Twitter Académiva V2
library(tidyverse)        # Manejo de datos y gráficas
library(lubridate)        # Manejo de fechas
library(svDialogs)        # Cuadros de diálogo y formularios
locale(date_names = "en", date_format = "%AD", time_format = "%AT",
  decimal_mark = ".", grouping_mark = ",", tz = "Europe/Berlin",
  encoding = "UTF-8", asciify = FALSE)
Sys.setlocale(category = "LC_ALL", locale = "english")

```

## Importamos funciones

```{r functions}
source ("share_functions.R")              # Funciones generales
source ("share_functions_API_academic.R") # Funciones para la API Academic
```

## Entorno por defecto

```{r environment}
## Entorno por defecto. No tocar salvo que se quiera usar otro entorno
dataset_name <- params$dataset_name    # Nombre del dataset
search <- params$search                # datos que se quieren bajar
max_users <- params$max_users          # Máximo número de tweets. 
start_time <- params$start_time        # Formato YYYY-MM-DD. Por defecto fecha de creación de Twitter
end_time <- params$end_time            # Formato YYYY-MM-DD. Por defecto hora actuual
data_path <- paste0("../datos/",dataset_name,"/") # Directorio de datos
file_out <- paste0(data_path,str_match(params$file_users, "(.*)\\..*$")[,2])
# Check si existe ya el dataset
check_dataset_exist(file_out)
file_users <- paste0(data_path,params$file_users)
key_file <- "../keys/key_academic_default.txt"  # Clave por defecto
if (!file.exists(key_file)) {
 stop("There is no default keyfile, the notebook cfg_API_academic.Rmd must be run once before")
}
temporal_file = "tmp"
size_block <- 10000
```

## Autenticación en OAuth

```{r authentication}
keys <- read.csv (file = key_file, header = FALSE)
```

## Descargar Tweets

```{r search_tweets, message=FALSE, warning=FALSE}
start_run_time = Sys.time ()
end_time <- check_dates (params$start_time, params$end_time )
end_tweets <- paste0(gsub(" ","T",end_time),"z") # Formato para función get_all_tweets
users<-paste(readLines(file_users),collapse=" ") 
users <-   strsplit(users," ")[[1]]
append <- FALSE

# Borramos ficheros temporales
if (file.exists(temporal_file)) {
  unlink(temporal_file, recursive = TRUE)
}
for (user in users) {
  id_user <- get_user_id(user, bearer_token = keys)
  if (params$search == "profile") {
    profile <- get_user_profile (id_user, bearer_token = keys) %>%
       mutate (net = NA)
    users_nor_df <- parser_users_API_academic (profile,"NA")
    write_csv (users_nor_df,paste0 (file_out,"_profiles.csv"), append = append)
    append = TRUE
  }
  if (params$search == "favorites") {
    tweets <- get_liked_tweets(id_users, bearer_token = keys) %>%
      mutate (liked_from = user)
    tweets_nor_df <- parser_tweets_basic_API_academic (tweets)
    write_csv (tweets_nor_df,paste0 (file_out,"_favorites.csv"), append = append)
    append = TRUE
  }
  if (params$search == "followers") {
    profile <- get_user_followers (id_user, bearer_token = keys)  %>%
       mutate (net = user)
    users_nor_df <- parser_users_API_academic (profile,"follower")
    write_csv (users_nor_df,paste0 (file_out,"_followers.csv"), append = append)
    append = TRUE
  }    
  if (params$search == "following") {
    profile <- get_user_following (id_user, bearer_token = keys) %>%
       mutate (net = user)
    users_nor_df <- parser_users_API_academic (profile,"following") 
    write_csv (users_nor_df,paste0 (file_out,"_following.csv"), append = append)
    append = TRUE
  } 
}
end_run_time = Sys.time ()
print (paste ( "inicio: ",start_run_time))
print (paste ( "fin: ",end_run_time))
```
